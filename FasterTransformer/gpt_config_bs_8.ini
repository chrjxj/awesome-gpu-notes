[ft_instance_hyperparameter]
max_batch_size=8 ; Use for allocate the buffer
max_seq_len=128 ; Use for allocate the buffer
layer_para_batch_size=8
candidate_num=1 ; k value for top k sampling
probability_threshold=0.0 ; p value for top p sampling
temperature=1.0 ; Use for sampling
tensor_para_size=1 ;4
layer_para_size=1 ;2
is_half=1
is_fuse_QKV=1
; model_name=gpt_124M
; model_name=gpt_175B
; model_name=self_defined
; model_name=megatron_345M
model_name=gpt_1558M

; model_name=megatron_6.7B
; model_path_prefix=./models/megatron-models/c-model/6.7b/
; model_path_prefix=./models/openai-gpt-models/c-model/124m/
; model_path_prefix=./models/megatron-models/c-model/345m/

model_path_prefix=/local/Documents/FasterTransformer-4.0/models/openai-gpt-models/c-model/1558M/

[request]
request_batch_size=8 # determine by the request
request_input_len=8 # determine by the request
request_output_len=32 # determine by the request

[gpt_124M]
head_num=12
size_per_head=64
vocab_size=50257
decoder_layers=12


[gpt_1558M]
head_num=12
size_per_head=64
vocab_size=50257
decoder_layers=48

[gpt_175B]
head_num=96
size_per_head=128
vocab_size=51200
decoder_layers=96

[self_defined]
head_num=16
size_per_head=64
vocab_size=30000
decoder_layers=12

[megatron_345M]
head_num=16
size_per_head=64
vocab_size=50304
decoder_layers=24

[megatron_6.7B]
head_num=32
size_per_head=128
vocab_size=51200
decoder_layers=32
