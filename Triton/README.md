# TensorRT


## Documents

- Triton Inference Server: [code and document](https://github.com/triton-inference-server/server)
- [Website](https://developer.nvidia.com/nvidia-triton-inference-server)
- [Overview](https://resources.nvidia.com/en-us-triton-inference-server/triton-technical-overview)
- [r21.06 doc](https://github.com/triton-inference-server/server/blob/r21.06/README.md#documentation)

- Triton Client Libraries and Examples: [code and document](https://github.com/triton-inference-server/client)


- model_analyzer

* https://github.com/triton-inference-server/model_analyzer

## GTC Talks and Workshops

- Easily Deploy AI Deep Learning Models at Scale with Triton Inference Server [link](https://gtc21.event.nvidia.com/media/t/1_ku5rh38w/204678073)

- Triton Deployment at Scale with Multi-Instance-GPU (MIG) and Kubernetes [link](https://gtc21.event.nvidia.com/media/Triton%20Deployment%20at%20Scale%20with%20Multi-Instance-GPU%20(MIG)%20and%20Kubernetes%20%5BS31724%5D/1_ipfgvoet/204678073)

