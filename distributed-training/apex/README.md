# APEX and Mixed Precision Training


### Talks and workshops

| Title  | Resource | Description |
|:-----|:-----------|:-----------|
| GTC2019, Session 9143, mixed precision Training | [record](https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=s9143-mixed+precision+training+of+deep+neural+networks) | mixed precision from a general perspective |
| GTC2019, Session 9998, APEX in PyTorch | [record](https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=s9998-automatic+mixed+precision+in+pytorch) | Pytorch-specific perspective |



### Document and papers

* Automatic mixed precision, [home page](https://developer.nvidia.com/automatic-mixed-precision)  
* Training with Mixed precision, [full document](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)
* Automatic Mixed Precision for NVIDIA Tensor Core Architecture in TensorFlow, [blog](https://developer.nvidia.com/blog/nvidia-automatic-mixed-precision-tensorflow/)
* Source code implementations, including mixed precision: https://developer.nvidia.com/deep-learning-examples 


Papers: 

1. [Large Scale Language Modeling: Converging on 40GB of Text in Four Hours, NVIDIA](https://arxiv.org/abs/1808.01371)
1. [Scaling Neural Machine Translation, Facebook](https://arxiv.org/abs/1806.00187): "shows that reduced precision and large batch training can speedup training by nearly
5x on a single 8-GPU machine"
1. [Mixed Precision Training](https://arxiv.org/abs/1710.03740), ICLR 2018 

### Reference

* initially folked from https://github.com/mcarilli/mixed_precision_references
