{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8033b2eb",
   "metadata": {},
   "source": [
    "# Deploying Quantization Aware Trained models in INT8 using Torch-TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec3ca7",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Quantization Aware training (QAT) simulates quantization during training by quantizing weights and activation layers. This will help to reduce the loss in accuracy when we convert the network trained in FP32 to INT8 for faster inference. QAT introduces additional nodes in the graph which will be used to learn the dynamic ranges of weights and activation layers. In this notebook, we illustrate the following steps from training to inference of a QAT model in Torch-TensorRT.\n",
    "\n",
    "1. [Requirements](#1)\n",
    "2. [VGG16 Overview](#2)\n",
    "3. [Training a baseline VGG16 model](#3)\n",
    "4. [Apply Quantization](#4)\n",
    "5. [Model calibration](#5)\n",
    "6. [Quantization Aware training](#6)\n",
    "7. [Export to Torchscript](#7)\n",
    "8. [Inference using Torch-TensorRT](#8)\n",
    "8. [References](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79655ea8",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "##  1. Requirements\n",
    "Please install the <a href=\"https://github.com/NVIDIA/Torch-TensorRT/tree/master/examples/int8/training/vgg16#prequisites\">required dependencies</a> and import these libraries accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14a72941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (7.7.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.6.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (1.1.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.0)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.22)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.4)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.9.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.2.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.8/site-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.12.1)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.3.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.1.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.12.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.9)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.3)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host=files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6493e915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch_tensorrt\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import pytorch_quantization\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import quant_modules\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
    "from pytorch_quantization import calib\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(pytorch_quantization.__version__)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"../examples/int8/training/vgg16\")\n",
    "from vgg16 import vgg16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5060a",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "##  2. VGG16 Overview\n",
    "### Very Deep Convolutional Networks for Large-Scale Image Recognition\n",
    "VGG is one of the earliest family of image classification networks that first used small (3x3) convolution filters and achieved significant improvements on ImageNet recognition challenge. The network architecture looks as follows\n",
    "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\">\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5afc49",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "##  3. Training a baseline VGG16 model\n",
    "We train VGG16 on CIFAR10 dataset. Define training and testing datasets and dataloaders. This will download the CIFAR 10 data in your `data` directory. Data preprocessing is performed using `torchvision` transforms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2c4c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8dc24320bb4749b5fac81ea689b3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# ========== Define Training dataset and dataloaders =============#\n",
    "training_dataset = datasets.CIFAR10(root='./data',\n",
    "                                        train=True,\n",
    "                                        download=True,\n",
    "                                        transform=transforms.Compose([\n",
    "                                            transforms.RandomCrop(32, padding=4),\n",
    "                                            transforms.RandomHorizontalFlip(),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "                                        ]))\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(training_dataset,\n",
    "                                                      batch_size=32,\n",
    "                                                      shuffle=True,\n",
    "                                                      num_workers=2)\n",
    "\n",
    "# ========== Define Testing dataset and dataloaders =============#\n",
    "testing_dataset = datasets.CIFAR10(root='./data',\n",
    "                                   train=False,\n",
    "                                   download=True,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "                                   ]))\n",
    "\n",
    "testing_dataloader = torch.utils.data.DataLoader(testing_dataset,\n",
    "                                                 batch_size=16,\n",
    "                                                 shuffle=False,\n",
    "                                                 num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd092b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, crit, opt, epoch):\n",
    "#     global writer\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch, (data, labels) in enumerate(dataloader):\n",
    "        data, labels = data.cuda(), labels.cuda(non_blocking=True)\n",
    "        opt.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = crit(out, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch % 500 == 499:\n",
    "            print(\"Batch: [%5d | %5d] loss: %.3f\" % (batch + 1, len(dataloader), running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "def test(model, dataloader, crit, epoch):\n",
    "    global writer\n",
    "    global classes\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss = 0.0\n",
    "    class_probs = []\n",
    "    class_preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data, labels = data.cuda(), labels.cuda(non_blocking=True)\n",
    "            out = model(data)\n",
    "            loss += crit(out, labels)\n",
    "            preds = torch.max(out, 1)[1]\n",
    "            class_probs.append([F.softmax(i, dim=0) for i in out])\n",
    "            class_preds.append(preds)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "    test_preds = torch.cat(class_preds)\n",
    "\n",
    "    return loss / total, correct / total\n",
    "\n",
    "def save_checkpoint(state, ckpt_path=\"checkpoint.pth\"):\n",
    "    torch.save(state, ckpt_path)\n",
    "    print(\"Checkpoint saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a86cc",
   "metadata": {},
   "source": [
    "*Define the VGG model that we are going to perfom QAT on.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c564b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR 10 has 10 classes\n",
    "model = vgg16(num_classes=len(classes), init_weights=False)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc00452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Learning rate\n",
    "lr = 0.1\n",
    "state = {}\n",
    "state[\"lr\"] = lr\n",
    "\n",
    "# Use cross entropy loss for classification and SGD optimizer\n",
    "crit = nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(model.parameters(), lr=state[\"lr\"], momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# Adjust learning rate based on epoch number\n",
    "def adjust_lr(optimizer, epoch):\n",
    "    global state\n",
    "    new_lr = lr * (0.5**(epoch // 12)) if state[\"lr\"] > 1e-7 else state[\"lr\"]\n",
    "    if new_lr != state[\"lr\"]:\n",
    "        state[\"lr\"] = new_lr\n",
    "        print(\"Updating learning rate: {}\".format(state[\"lr\"]))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = state[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d80865a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [    1 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 13.022\n",
      "Batch: [ 1000 |  1563] loss: 11.059\n",
      "Batch: [ 1500 |  1563] loss: 10.604\n",
      "Test Loss: 0.12732 Test Acc: 19.23%\n",
      "Epoch: [    2 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 10.226\n",
      "Batch: [ 1000 |  1563] loss: 10.073\n",
      "Batch: [ 1500 |  1563] loss: 10.016\n",
      "Test Loss: 0.11906 Test Acc: 18.63%\n",
      "Epoch: [    3 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.945\n",
      "Batch: [ 1000 |  1563] loss: 9.919\n",
      "Batch: [ 1500 |  1563] loss: 9.959\n",
      "Test Loss: 0.12927 Test Acc: 18.03%\n",
      "Epoch: [    4 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.788\n",
      "Batch: [ 1000 |  1563] loss: 9.967\n",
      "Batch: [ 1500 |  1563] loss: 9.879\n",
      "Test Loss: 0.11635 Test Acc: 21.93%\n",
      "Epoch: [    5 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.894\n",
      "Batch: [ 1000 |  1563] loss: 9.879\n",
      "Batch: [ 1500 |  1563] loss: 9.842\n",
      "Test Loss: 0.11638 Test Acc: 21.07%\n",
      "Epoch: [    6 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.815\n",
      "Batch: [ 1000 |  1563] loss: 9.792\n",
      "Batch: [ 1500 |  1563] loss: 9.756\n",
      "Test Loss: 0.11913 Test Acc: 19.42%\n",
      "Epoch: [    7 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.838\n",
      "Batch: [ 1000 |  1563] loss: 9.807\n",
      "Batch: [ 1500 |  1563] loss: 9.537\n",
      "Test Loss: 0.12355 Test Acc: 21.72%\n",
      "Epoch: [    8 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.485\n",
      "Batch: [ 1000 |  1563] loss: 9.625\n",
      "Batch: [ 1500 |  1563] loss: 9.257\n",
      "Test Loss: 0.10985 Test Acc: 27.23%\n",
      "Epoch: [    9 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.301\n",
      "Batch: [ 1000 |  1563] loss: 9.275\n",
      "Batch: [ 1500 |  1563] loss: 9.341\n",
      "Test Loss: 0.11734 Test Acc: 22.95%\n",
      "Epoch: [   10 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.157\n",
      "Batch: [ 1000 |  1563] loss: 9.003\n",
      "Batch: [ 1500 |  1563] loss: 9.124\n",
      "Test Loss: 0.10777 Test Acc: 27.83%\n",
      "Epoch: [   11 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 9.070\n",
      "Batch: [ 1000 |  1563] loss: 8.997\n",
      "Batch: [ 1500 |  1563] loss: 9.073\n",
      "Test Loss: 0.10530 Test Acc: 26.50%\n",
      "Epoch: [   12 /    25] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 8.956\n",
      "Batch: [ 1000 |  1563] loss: 9.058\n",
      "Batch: [ 1500 |  1563] loss: 8.870\n",
      "Test Loss: 0.11095 Test Acc: 27.12%\n",
      "Updating learning rate: 0.05\n",
      "Epoch: [   13 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 8.370\n",
      "Batch: [ 1000 |  1563] loss: 8.028\n",
      "Batch: [ 1500 |  1563] loss: 7.902\n",
      "Test Loss: 0.09420 Test Acc: 37.75%\n",
      "Epoch: [   14 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 7.743\n",
      "Batch: [ 1000 |  1563] loss: 7.599\n",
      "Batch: [ 1500 |  1563] loss: 7.391\n",
      "Test Loss: 0.08991 Test Acc: 45.65%\n",
      "Epoch: [   15 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 7.172\n",
      "Batch: [ 1000 |  1563] loss: 6.986\n",
      "Batch: [ 1500 |  1563] loss: 7.245\n",
      "Test Loss: 0.07708 Test Acc: 54.56%\n",
      "Epoch: [   16 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 6.616\n",
      "Batch: [ 1000 |  1563] loss: 6.559\n",
      "Batch: [ 1500 |  1563] loss: 6.287\n",
      "Test Loss: 0.06767 Test Acc: 61.78%\n",
      "Epoch: [   17 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 6.046\n",
      "Batch: [ 1000 |  1563] loss: 5.887\n",
      "Batch: [ 1500 |  1563] loss: 5.809\n",
      "Test Loss: 0.08510 Test Acc: 55.12%\n",
      "Epoch: [   18 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 5.544\n",
      "Batch: [ 1000 |  1563] loss: 5.478\n",
      "Batch: [ 1500 |  1563] loss: 5.426\n",
      "Test Loss: 0.05950 Test Acc: 68.95%\n",
      "Epoch: [   19 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 5.115\n",
      "Batch: [ 1000 |  1563] loss: 5.241\n",
      "Batch: [ 1500 |  1563] loss: 5.139\n",
      "Test Loss: 0.06063 Test Acc: 67.87%\n",
      "Epoch: [   20 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 4.887\n",
      "Batch: [ 1000 |  1563] loss: 4.730\n",
      "Batch: [ 1500 |  1563] loss: 4.720\n",
      "Test Loss: 0.05433 Test Acc: 71.39%\n",
      "Epoch: [   21 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 4.566\n",
      "Batch: [ 1000 |  1563] loss: 4.590\n",
      "Batch: [ 1500 |  1563] loss: 4.464\n",
      "Test Loss: 0.05335 Test Acc: 72.19%\n",
      "Epoch: [   22 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 4.340\n",
      "Batch: [ 1000 |  1563] loss: 4.211\n",
      "Batch: [ 1500 |  1563] loss: 4.265\n",
      "Test Loss: 0.05208 Test Acc: 74.54%\n",
      "Epoch: [   23 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 4.110\n",
      "Batch: [ 1000 |  1563] loss: 4.041\n",
      "Batch: [ 1500 |  1563] loss: 4.079\n",
      "Test Loss: 0.04619 Test Acc: 76.83%\n",
      "Epoch: [   24 /    25] LR: 0.050000\n",
      "Batch: [  500 |  1563] loss: 3.865\n",
      "Batch: [ 1000 |  1563] loss: 3.881\n",
      "Batch: [ 1500 |  1563] loss: 3.924\n",
      "Test Loss: 0.04901 Test Acc: 75.82%\n",
      "Updating learning rate: 0.025\n",
      "Epoch: [   25 /    25] LR: 0.025000\n",
      "Batch: [  500 |  1563] loss: 3.155\n",
      "Batch: [ 1000 |  1563] loss: 2.938\n",
      "Batch: [ 1500 |  1563] loss: 2.988\n",
      "Test Loss: 0.03391 Test Acc: 81.88%\n",
      "Checkpoint saved\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 25 epochs to get ~80% accuracy.\n",
    "num_epochs=25\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_lr(opt, epoch)\n",
    "    print('Epoch: [%5d / %5d] LR: %f' % (epoch + 1, num_epochs, state[\"lr\"]))\n",
    "\n",
    "    train(model, training_dataloader, crit, opt, epoch)\n",
    "    test_loss, test_acc = test(model, testing_dataloader, crit, epoch)\n",
    "\n",
    "    print(\"Test Loss: {:.5f} Test Acc: {:.2f}%\".format(test_loss, 100 * test_acc))\n",
    "    \n",
    "save_checkpoint({'epoch': epoch + 1,\n",
    "                 'model_state_dict': model.state_dict(),\n",
    "                 'acc': test_acc,\n",
    "                 'opt_state_dict': opt.state_dict(),\n",
    "                 'state': state},\n",
    "                ckpt_path=\"vgg16_base_ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1044537",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "##  4. Apply Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b7f4e",
   "metadata": {},
   "source": [
    "`quant_modules.initialize()` will ensure quantized version of modules will be called instead of original modules. For example, when you define a model with convolution, linear, pooling layers, `QuantConv2d`, `QuantLinear` and `QuantPooling` will be called. `QuantConv2d` basically wraps quantizer nodes around inputs and weights of regular `Conv2d`. Please refer to all the <a href=\"https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization/pytorch_quantization/nn/modules\">quantized modules</a> in pytorch-quantization toolkit for more information. A `QuantConv2d` is represented in `pytorch-quantization` toolkit as follows.\n",
    "\n",
    "```\n",
    "def forward(self, input):\n",
    "        # the actual quantization happens in the next level of the class hierarchy\n",
    "        quant_input, quant_weight = self._quant(input)\n",
    "\n",
    "        if self.padding_mode == 'circular':\n",
    "            expanded_padding = ((self.padding[1] + 1) // 2, self.padding[1] // 2,\n",
    "                                (self.padding[0] + 1) // 2, self.padding[0] // 2)\n",
    "            output = F.conv2d(F.pad(quant_input, expanded_padding, mode='circular'),\n",
    "                              quant_weight, self.bias, self.stride,\n",
    "                              _pair(0), self.dilation, self.groups)\n",
    "        else:\n",
    "            output = F.conv2d(quant_input, quant_weight, self.bias, self.stride, self.padding, self.dilation,\n",
    "                              self.groups)\n",
    "\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "985dc59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_modules.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "164ce8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the regular conv, FC layers will be converted to their quantozed counterparts due to quant_modules.initialize()\n",
    "qat_model = vgg16(num_classes=len(classes), init_weights=False)\n",
    "qat_model = qat_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e5f7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16_base_ckpt is the checkpoint generated from Step 3 : Training a baseline VGG16 model.\n",
    "ckpt = torch.load(\"./vgg16_base_ckpt\")\n",
    "modified_state_dict={}\n",
    "for key, val in ckpt[\"model_state_dict\"].items():\n",
    "    # Remove 'module.' from the key names\n",
    "    if key.startswith('module'):\n",
    "        modified_state_dict[key[7:]] = val\n",
    "    else:\n",
    "        modified_state_dict[key] = val\n",
    "\n",
    "# Load the pre-trained checkpoint\n",
    "qat_model.load_state_dict(modified_state_dict)\n",
    "opt.load_state_dict(ckpt[\"opt_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8a74e8",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "##  5. Model Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a321f9",
   "metadata": {},
   "source": [
    "The quantizer nodes introduced in the model around desired layers capture the dynamic range (min_value, max_value) that is observed by the layer. Calibration is the process of computing the dynamic range of these layers by passing calibration data, which is usually a subset of training or validation data. There are different ways of calibration: `max`, `histogram` and `entropy`. We use `max` calibration technique as it is simple and effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "039423dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_amax(model, **kwargs):\n",
    "    # Load calib result\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                    module.load_calib_amax()\n",
    "                else:\n",
    "                    module.load_calib_amax(**kwargs)\n",
    "            print(F\"{name:40}: {module}\")\n",
    "    model.cuda()\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "    \"\"\"Feed data to the network and collect statistics\"\"\"\n",
    "    # Enable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.disable_quant()\n",
    "                module.enable_calib()\n",
    "            else:\n",
    "                module.disable()\n",
    "\n",
    "    # Feed data to the network for collecting stats\n",
    "    for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "        model(image.cuda())\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "\n",
    "    # Disable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.enable_quant()\n",
    "                module.disable_calib()\n",
    "            else:\n",
    "                module.enable()\n",
    "\n",
    "def calibrate_model(model, model_name, data_loader, num_calib_batch, calibrator, hist_percentile, out_dir):\n",
    "    \"\"\"\n",
    "        Feed data to the network and calibrate.\n",
    "        Arguments:\n",
    "            model: classification model\n",
    "            model_name: name to use when creating state files\n",
    "            data_loader: calibration data set\n",
    "            num_calib_batch: amount of calibration passes to perform\n",
    "            calibrator: type of calibration to use (max/histogram)\n",
    "            hist_percentile: percentiles to be used for historgram calibration\n",
    "            out_dir: dir to save state files in\n",
    "    \"\"\"\n",
    "\n",
    "    if num_calib_batch > 0:\n",
    "        print(\"Calibrating model\")\n",
    "        with torch.no_grad():\n",
    "            collect_stats(model, data_loader, num_calib_batch)\n",
    "\n",
    "        if not calibrator == \"histogram\":\n",
    "            compute_amax(model, method=\"max\")\n",
    "            calib_output = os.path.join(\n",
    "                out_dir,\n",
    "                F\"{model_name}-max-{num_calib_batch*data_loader.batch_size}.pth\")\n",
    "            torch.save(model.state_dict(), calib_output)\n",
    "        else:\n",
    "            for percentile in hist_percentile:\n",
    "                print(F\"{percentile} percentile calibration\")\n",
    "                compute_amax(model, method=\"percentile\")\n",
    "                calib_output = os.path.join(\n",
    "                    out_dir,\n",
    "                    F\"{model_name}-percentile-{percentile}-{num_calib_batch*data_loader.batch_size}.pth\")\n",
    "                torch.save(model.state_dict(), calib_output)\n",
    "\n",
    "            for method in [\"mse\", \"entropy\"]:\n",
    "                print(F\"{method} calibration\")\n",
    "                compute_amax(model, method=method)\n",
    "                calib_output = os.path.join(\n",
    "                    out_dir,\n",
    "                    F\"{model_name}-{method}-{num_calib_batch*data_loader.batch_size}.pth\")\n",
    "                torch.save(model.state_dict(), calib_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78504a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 56.66it/s]\n",
      "W0609 23:20:44.398325 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.399407 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.400234 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.401015 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.401801 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.402570 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.403513 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.404308 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.405080 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.405840 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.406595 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.407414 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.408243 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.408996 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.409533 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.410069 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.410615 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.411172 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.411724 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.412258 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.412796 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.413318 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.413860 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.414386 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.414971 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.415519 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.416069 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.416610 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.417162 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.417708 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.418229 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.418769 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.419316 140329249969984 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W0609 23:20:44.420032 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.420595 140329249969984 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "W0609 23:20:44.426763 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([64, 1, 1, 1]).\n",
      "W0609 23:20:44.428088 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.428967 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([64, 1, 1, 1]).\n",
      "W0609 23:20:44.430046 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.431032 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([128, 1, 1, 1]).\n",
      "W0609 23:20:44.432111 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.433086 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([128, 1, 1, 1]).\n",
      "W0609 23:20:44.434151 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.435142 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([256, 1, 1, 1]).\n",
      "W0609 23:20:44.436214 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.437157 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([256, 1, 1, 1]).\n",
      "W0609 23:20:44.438213 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.439216 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([256, 1, 1, 1]).\n",
      "W0609 23:20:44.440278 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.441206 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([512, 1, 1, 1]).\n",
      "W0609 23:20:44.442185 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.443125 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([512, 1, 1, 1]).\n",
      "W0609 23:20:44.444118 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.445011 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([512, 1, 1, 1]).\n",
      "W0609 23:20:44.446027 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.446922 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([512, 1, 1, 1]).\n",
      "W0609 23:20:44.447914 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.448791 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([512, 1, 1, 1]).\n",
      "W0609 23:20:44.449776 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.450387 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([512, 1, 1, 1]).\n",
      "W0609 23:20:44.451649 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.452558 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.453498 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([4096, 1]).\n",
      "W0609 23:20:44.454500 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.455455 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([4096, 1]).\n",
      "W0609 23:20:44.456465 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W0609 23:20:44.457370 140329249969984 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([10, 1]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=2.7537 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.0._weight_quantizer            : TensorQuantizer(8bit fake axis=0 amax=[0.0246, 2.9098](64) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.3._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=28.3188 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.3._weight_quantizer            : TensorQuantizer(8bit fake axis=0 amax=[0.1431, 1.6453](64) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.7._input_quantizer             : TensorQuantizer(8bit fake per-tensor amax=15.5314 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.7._weight_quantizer            : TensorQuantizer(8bit fake axis=0 amax=[0.0489, 0.9367](128) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.10._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=8.2504 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.10._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0603, 0.7806](128) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.14._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=11.2396 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.14._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0480, 0.7302](256) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.17._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=11.9798 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.17._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0392, 0.5425](256) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.20._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=8.3537 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.20._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0403, 0.4161](256) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.24._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=7.5847 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.24._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0204, 0.2580](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.27._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=8.4515 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.27._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0191, 0.1245](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.30._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=3.2038 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.30._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0071, 0.1317](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.34._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=3.8242 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.34._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0079, 0.2088](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.37._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=2.1710 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.37._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0067, 0.3757](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.40._input_quantizer            : TensorQuantizer(8bit fake per-tensor amax=2.8133 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "features.40._weight_quantizer           : TensorQuantizer(8bit fake axis=0 amax=[0.0023, 0.5639](512) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "avgpool._input_quantizer                : TensorQuantizer(8bit fake per-tensor amax=4.3074 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "classifier.0._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=4.3074 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "classifier.0._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.0027, 0.6115](4096) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "classifier.3._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=5.2620 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "classifier.3._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.0016, 0.5163](4096) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "classifier.6._input_quantizer           : TensorQuantizer(8bit fake per-tensor amax=7.2716 calibrator=MaxCalibrator scale=1.0 quant)\n",
      "classifier.6._weight_quantizer          : TensorQuantizer(8bit fake axis=0 amax=[0.3425, 0.4469](10) calibrator=MaxCalibrator scale=1.0 quant)\n"
     ]
    }
   ],
   "source": [
    "#Calibrate the model using max calibration technique.\n",
    "with torch.no_grad():\n",
    "    calibrate_model(\n",
    "        model=qat_model,\n",
    "        model_name=\"vgg16\",\n",
    "        data_loader=training_dataloader,\n",
    "        num_calib_batch=32,\n",
    "        calibrator=\"max\",\n",
    "        hist_percentile=[99.9, 99.99, 99.999, 99.9999],\n",
    "        out_dir=\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa0c109",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "##  6. Quantization Aware Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe8ec11",
   "metadata": {},
   "source": [
    "In this phase, we finetune the model weights and leave the quantizer node values frozen. The dynamic ranges for each layer obtained from the calibration are kept constant while the weights of the model are finetuned to be close to the accuracy of original FP32 model (model without quantizer nodes) is preserved. Usually the finetuning of QAT model should be quick compared to the full training of the original model. Use QAT to fine-tune for around 10% of the original training schedule with an annealing learning-rate. Please refer to <a href=\"https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/\">Achieving FP32 Accuracy for INT8 Inference Using Quantization Aware Training with NVIDIA TensorRT</a> for detailed recommendations. For this VGG model, it is enough to finetune for 1 epoch to get acceptable accuracy. \n",
    "During finetuning with QAT, the quantization is applied as a composition of `max`, `clamp`, `round` and `mul` ops. \n",
    "```\n",
    "# amax is absolute maximum value for an input\n",
    "# The upper bound for integer quantization (127 for int8)\n",
    "max_bound = torch.tensor((2.0**(num_bits - 1 + int(unsigned))) - 1.0, device=amax.device)\n",
    "scale = max_bound / amax\n",
    "outputs = torch.clamp((inputs * scale).round_(), min_bound, max_bound)\n",
    "```\n",
    "<a href=\"https://github.com/NVIDIA/TensorRT/blob/8.0.1/tools/pytorch-quantization/pytorch_quantization/tensor_quant.py\">tensor_quant function</a> in `pytorch_quantization` toolkit is responsible for the above tensor quantization. Usually, per channel quantization is recommended for weights, while per tensor quantization is recommended for activations in a network.\n",
    "During inference, we use `torch.fake_quantize_per_tensor_affine` and `torch.fake_quantize_per_channel_affine` to perform quantization as this is easier to convert into corresponding TensorRT operators. Please refer to next sections for more details on how these operators are exported in torchscript and converted in Torch-TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f28d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [    1 /     2] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 2.703\n",
      "Batch: [ 1000 |  1563] loss: 2.712\n",
      "Batch: [ 1500 |  1563] loss: 2.645\n",
      "Test Loss: 0.03265 Test Acc: 82.93%\n",
      "Epoch: [    2 /     2] LR: 0.100000\n",
      "Batch: [  500 |  1563] loss: 2.662\n",
      "Batch: [ 1000 |  1563] loss: 2.726\n",
      "Batch: [ 1500 |  1563] loss: 2.773\n",
      "Test Loss: 0.03270 Test Acc: 82.94%\n",
      "Checkpoint saved\n"
     ]
    }
   ],
   "source": [
    "# Finetune the QAT model for 1 epoch\n",
    "num_epochs=2\n",
    "for epoch in range(num_epochs):\n",
    "    adjust_lr(opt, epoch)\n",
    "    print('Epoch: [%5d / %5d] LR: %f' % (epoch + 1, num_epochs, state[\"lr\"]))\n",
    "\n",
    "    train(qat_model, training_dataloader, crit, opt, epoch)\n",
    "    test_loss, test_acc = test(qat_model, testing_dataloader, crit, epoch)\n",
    "\n",
    "    print(\"Test Loss: {:.5f} Test Acc: {:.2f}%\".format(test_loss, 100 * test_acc))\n",
    "    \n",
    "save_checkpoint({'epoch': epoch + 1,\n",
    "                 'model_state_dict': qat_model.state_dict(),\n",
    "                 'acc': test_acc,\n",
    "                 'opt_state_dict': opt.state_dict(),\n",
    "                 'state': state},\n",
    "                ckpt_path=\"vgg16_qat_ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4dcaa2",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "##  7. Export to Torchscript\n",
    "Export the model to Torch script. Trace the model and convert it into torchscript for deployment. To learn more about Torchscript, please refer to https://pytorch.org/docs/stable/jit.html. Setting `quant_nn.TensorQuantizer.use_fb_fake_quant = True` enables the QAT model to use `torch.fake_quantize_per_tensor_affine` and `torch.fake_quantize_per_channel_affine` operators instead of `tensor_quant` function to export quantization operators. In torchscript, they are represented as `aten::fake_quantize_per_tensor_affine` and `aten::fake_quantize_per_channel_affine`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d34f526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0609 23:25:12.739173 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.741839 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.748401 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.749318 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.762119 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.763071 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.766882 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.767826 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.782656 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.783537 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.787470 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.788382 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.803482 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.804278 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.808462 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.809342 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.824748 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.825543 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.829492 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.830257 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.842592 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.843380 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.847686 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.848431 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.860903 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.861919 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.865693 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.867046 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.882002 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.882943 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.886741 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.887656 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.900683 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.901618 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.905374 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.906249 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.918111 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.918910 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.922056 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.922988 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.936310 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.937205 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.940337 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.941261 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.952223 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.953038 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.956117 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.956880 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.966802 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.967653 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.971300 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.971962 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.987456 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.988173 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.993829 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.994649 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.997814 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:12.998586 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.008123 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.009006 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.012133 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.012932 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.022909 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.023632 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.026707 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.027485 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.118395 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.119496 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.122673 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.123428 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.134101 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.134867 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.137901 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.138699 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.150883 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.151647 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.154706 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.155535 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.165261 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.166075 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.169259 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.170050 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.185166 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.186007 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.189103 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.190331 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.199876 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.201183 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.204226 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.204968 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.215666 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.216527 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.219678 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.220362 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.233136 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.233833 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.236889 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.237576 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.248495 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.249277 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.252271 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.252966 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.263510 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.264126 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.267761 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.268435 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.280923 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.281763 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.284817 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.285630 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.296615 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.297350 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.300600 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.301334 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.312277 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.312960 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.315965 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.316713 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.332816 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.333475 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.339118 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.340399 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.343922 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.345028 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.356225 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.356906 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.360014 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.360585 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.370063 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.370967 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.374277 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n",
      "E0609 23:25:13.374880 140329249969984 tensor_quantizer.py:120] Fake quantize mode doesn't use scale explicitly!\n"
     ]
    }
   ],
   "source": [
    "quant_nn.TensorQuantizer.use_fb_fake_quant = True\n",
    "with torch.no_grad():\n",
    "    data = iter(testing_dataloader)\n",
    "    images, _ = data.next()\n",
    "    jit_model = torch.jit.trace(qat_model, images.to(\"cuda\"))\n",
    "    torch.jit.save(jit_model, \"trained_vgg16_qat.jit.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341418a",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "##  8. Inference using Torch-TensorRT\n",
    "In this phase, we run the exported torchscript graph of VGG QAT using Torch-TensorRT. Torch-TensorRT is a Pytorch-TensorRT compiler which converts Torchscript graphs into TensorRT. TensorRT 8.0 supports inference of quantization aware trained models and introduces new APIs; `QuantizeLayer` and `DequantizeLayer`. We can observe the entire VGG QAT graph quantization nodes from the debug log of Torch-TensorRT. To enable debug logging, you can set `torch_tensorrt.logging.set_reportable_log_level(torch_tensorrt.logging.Level.Debug)`. For example, `QuantConv2d` layer from `pytorch_quantization` toolkit is represented as follows in Torchscript\n",
    "```\n",
    "%quant_input : Tensor = aten::fake_quantize_per_tensor_affine(%x, %636, %637, %638, %639)\n",
    "%quant_weight : Tensor = aten::fake_quantize_per_channel_affine(%394, %640, %641, %637, %638, %639)\n",
    "%input.2 : Tensor = aten::_convolution(%quant_input, %quant_weight, %395, %687, %688, %689, %643, %690, %642, %643, %643, %644, %644)\n",
    "```\n",
    "`aten::fake_quantize_per_*_affine` is converted into `QuantizeLayer` + `DequantizeLayer` in Torch-TensorRT internally. Please refer to <a href=\"https://github.com/NVIDIA/Torch-TensorRT/blob/master/core/conversion/converters/impl/quantization.cpp\">quantization op converters</a> in Torch-TensorRT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa7495e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Cannot infer input type from calcuations in graph for input x.2. Assuming it is Float32. If not, specify input type explicity\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG QAT accuracy using TensorRT: 82.94%\n"
     ]
    }
   ],
   "source": [
    "qat_model = torch.jit.load(\"trained_vgg16_qat.jit.pt\").eval()\n",
    "\n",
    "compile_spec = {\"inputs\": [torch_tensorrt.Input([16, 3, 32, 32])],\n",
    "                \"enabled_precisions\": torch.int8,\n",
    "                }\n",
    "trt_mod = torch_tensorrt.compile(qat_model, **compile_spec)\n",
    "\n",
    "test_loss, test_acc = test(trt_mod, testing_dataloader, crit, 0)\n",
    "print(\"VGG QAT accuracy using TensorRT: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df5a90e",
   "metadata": {},
   "source": [
    "### Performance benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eb2cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Helper function to benchmark the model\n",
    "def benchmark(model, input_shape=(1024, 1, 32, 32), dtype='fp32', nwarmup=50, nruns=1000):\n",
    "    input_data = torch.randn(input_shape)\n",
    "    input_data = input_data.to(\"cuda\")\n",
    "    if dtype=='fp16':\n",
    "        input_data = input_data.half()\n",
    "        \n",
    "    print(\"Warm up ...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(nwarmup):\n",
    "            features = model(input_data)\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"Start timing ...\")\n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, nruns+1):\n",
    "            start_time = time.time()\n",
    "            output = model(input_data)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            timings.append(end_time - start_time)\n",
    "            if i%100==0:\n",
    "                print('Iteration %d/%d, avg batch time %.2f ms'%(i, nruns, np.mean(timings)*1000))\n",
    "\n",
    "    print(\"Input shape:\", input_data.size())\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print('Average batch time: %.2f ms'%(np.mean(timings)*1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c2514ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "Iteration 100/1000, avg batch time 5.67 ms\n",
      "Iteration 200/1000, avg batch time 5.66 ms\n",
      "Iteration 300/1000, avg batch time 5.66 ms\n",
      "Iteration 400/1000, avg batch time 5.66 ms\n",
      "Iteration 500/1000, avg batch time 5.66 ms\n",
      "Iteration 600/1000, avg batch time 5.66 ms\n",
      "Iteration 700/1000, avg batch time 5.67 ms\n",
      "Iteration 800/1000, avg batch time 5.67 ms\n",
      "Iteration 900/1000, avg batch time 5.67 ms\n",
      "Iteration 1000/1000, avg batch time 5.67 ms\n",
      "Input shape: torch.Size([16, 3, 32, 32])\n",
      "Output shape: torch.Size([16, 10])\n",
      "Average batch time: 5.67 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(jit_model, input_shape=(16, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5378ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "Iteration 100/1000, avg batch time 2.30 ms\n",
      "Iteration 200/1000, avg batch time 2.17 ms\n",
      "Iteration 300/1000, avg batch time 2.12 ms\n",
      "Iteration 400/1000, avg batch time 2.09 ms\n",
      "Iteration 500/1000, avg batch time 2.08 ms\n",
      "Iteration 600/1000, avg batch time 2.12 ms\n",
      "Iteration 700/1000, avg batch time 2.10 ms\n",
      "Iteration 800/1000, avg batch time 2.09 ms\n",
      "Iteration 900/1000, avg batch time 2.08 ms\n",
      "Iteration 1000/1000, avg batch time 2.07 ms\n",
      "Input shape: torch.Size([16, 3, 32, 32])\n",
      "Output shape: torch.Size([16, 10])\n",
      "Average batch time: 2.07 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(trt_mod, input_shape=(16, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a5ec1c",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "##  9. References\n",
    "* <a href=\"https://arxiv.org/pdf/1409.1556.pdf\">Very Deep Convolution Networks for large scale Image Recognition</a>\n",
    "* <a href=\"https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/\">Achieving FP32 Accuracy for INT8 Inference Using Quantization Aware Training with NVIDIA TensorRT</a>\n",
    "* <a href=\"https://github.com/NVIDIA/Torch-TensorRT/tree/master/examples/int8/training/vgg16#quantization-aware-fine-tuning-for-trying-out-qat-workflows\">QAT workflow for VGG16</a>\n",
    "* <a href=\"https://github.com/NVIDIA/Torch-TensorRT/tree/master/examples/int8/qat\">Deploying VGG QAT model in C++ using Torch-TensorRT</a>\n",
    "* <a href=\"https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization\">Pytorch-quantization toolkit from NVIDIA</a>\n",
    "* <a href=\"https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/userguide.html\">Pytorch quantization toolkit userguide</a>\n",
    "* <a href=\"https://arxiv.org/pdf/2004.09602.pdf\">Quantization basics</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
